{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we used different resources:\n",
    "https://huggingface.co/docs/transformers/index\n",
    "https://huggingface.co/docs/transformers/tasks/question_answering\n",
    "https://huggingface.co/docs/transformers/model_doc/llama3 - https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct\n",
    "\n",
    "We want to fine-tune Llama3-70B with our data.\n",
    "As we understand right now, the traditional question-answer transformer needs:\n",
    "\t- context\n",
    "\t- question\n",
    "\t- answer found in context\n",
    "\n",
    "With mintaka we have no context, but instead wikidata entities. \n",
    "Do we find some transformer that takes question entities and answer entities separately?\n",
    "What kind of transformer do we need to do this?\n",
    "\n",
    "We need to convert all inputs to type string(at least the numerical answers, maybe more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --cache-dir=/home/user/tmp torch transformers==2.5.1 datasets accelerate --break-system-packages\n",
    "#transformers datasets accelerate torch deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "# import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the dataset from the mintaka data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('../dataset-generation/data/mintaka_dev_extended_preprocessed.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "formatted_data = []\n",
    "for entry in data:\n",
    "    if 'answer' in entry:\n",
    "        if isinstance(entry['answer'], list):\n",
    "            for i, ans in enumerate(entry['answer']):\n",
    "                if isinstance(ans, int):\n",
    "                    entry['answer'][i] = {\"answerType\": \"numerical\", \"answer\": ans, \"mention\": str(ans)}\n",
    "                elif isinstance(ans, str):\n",
    "                    entry['answer'][i] = {\"answerType\": \"text\", \"answer\": ans, \"mention\": ans}\n",
    "        elif isinstance(entry['answer'], int):\n",
    "            entry['answer'] = {\"answerType\": \"numerical\", \"answer\": entry['answer'], \"mention\": str(entry['answer'])}\n",
    "        elif isinstance(entry['answer'], str):\n",
    "            entry['answer'] = {\"answerType\": \"text\", \"answer\": entry['answer'], \"mention\": entry['answer']}\n",
    "        elif not isinstance(entry['answer'], dict):\n",
    "            entry['answer'] = {\"answerType\": None, \"answer\": [], \"mention\": None}\n",
    "    else:\n",
    "        entry['answer'] = {\"answerType\": None, \"answer\": [], \"mention\": None}\n",
    "\n",
    "    formatted_entry = {\n",
    "        \"id\": entry[\"id\"],\n",
    "        \"question\": entry[\"question\"],\n",
    "        \"translations\": entry[\"translations\"],\n",
    "        \"answer\": entry[\"answer\"][\"mention\"],\n",
    "        \"answer_translations\": [entity[\"label\"] for entity in entry[\"answer\"][\"answer\"] if type(entry[\"answer\"][\"answer\"]) == list ]\n",
    "    }\n",
    "    formatted_data.append(formatted_entry)\n",
    "    \n",
    "\n",
    "\n",
    "with open('./mintaka_dev_extended_formatted.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(formatted_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../dataset-generation/data/\"\n",
    "train_file = path_to_data+\"mintaka_train.json\"\n",
    "test_file = path_to_data+\"mintaka_test.json\"\n",
    "# dev_file = path_to_data+\"mintaka_dev.json\"\n",
    "# dev_file = path_to_data+\"mintaka_dev_extended.json\"\n",
    "dev_file = \"./mintaka_dev_extended_formatted.json\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": dev_file, \"validation\": dev_file})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we preprocess the data to prepare it for the Llama model\n",
    "The user needs to have been granted access to the Llama3.3 model on Huggingface while being logged in on the client system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre_trained_model = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess our dataset\n",
    "\n",
    "We need to separate the dataset into a \"question\" list and an \"answers\" list, which are symmetrically ordered and then encode them with the tokenizer which we loaded from llama3 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "\t# https://huggingface.co/transformers/v3.0.2/preprocessing.html\n",
    "\t# Look at 'Preprocessing pars of sentences'\n",
    "\t# encoded_input = tokenizer(\"How old are you?\", \"I'm 6 years old\")\n",
    "\t# print(encoded_input)\n",
    "\n",
    "\tbatch_questions = [\n",
    "\t\t#LIST OF ALL QUESTIONS IN ORDER\n",
    "\t]\n",
    "\t\n",
    "\tbatch_answers = [\n",
    "\t\t#LIST OF ALL ANSWERS IN SAME ORDER AS QUESTIONS\n",
    "\t]\n",
    "\n",
    "\tencoded_inputs = tokenizer(batch_questions, batch_answers)\n",
    "\n",
    "\treturn {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map and test our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset.map(preprocess_data, bached=True)\n",
    "\n",
    "print(processed_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, get_scheduler, AutoModelForCausalLM\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(pre_trained_model)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    #do stuff here with our loss function and backwards propagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
