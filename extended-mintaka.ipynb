{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we used different resources:\n",
    "https://huggingface.co/docs/transformers/index\n",
    "https://huggingface.co/docs/transformers/tasks/question_answering\n",
    "https://huggingface.co/docs/transformers/model_doc/llama3 - https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct\n",
    "\n",
    "We want to fine-tune Llama3-70B with our data.\n",
    "As we understand right now, the traditional question-answer transformer needs:\n",
    "\t- context\n",
    "\t- question\n",
    "\t- answer found in context\n",
    "\n",
    "With mintaka we have no context, but instead wikidata entities. \n",
    "Do we find some transformer that takes question entities and answer entities separately?\n",
    "What kind of transformer do we need to do this?\n",
    "\n",
    "We need to convert all inputs to type string(at least the numerical answers, maybe more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --cache-dir=/home/user/tmp torch transformers==2.5.1 datasets accelerate --break-system-packages\n",
    "#transformers datasets accelerate torch deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "# import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the dataset from the mintaka data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The json manipulaton is WIP, we need to transform the original data properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('../dataset-generation/data/mintaka_dev_extended_preprocessed.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "formatted_data = []\n",
    "for entry in data:\n",
    "    if 'answer' in entry:\n",
    "        if isinstance(entry['answer'], list):\n",
    "            for i, ans in enumerate(entry['answer']):\n",
    "                if isinstance(ans, int):\n",
    "                    entry['answer'][i] = {\"answerType\": \"numerical\", \"answer\": ans, \"mention\": str(ans)}\n",
    "                elif isinstance(ans, str):\n",
    "                    entry['answer'][i] = {\"answerType\": \"text\", \"answer\": ans, \"mention\": ans}\n",
    "        elif isinstance(entry['answer'], int):\n",
    "            entry['answer'] = {\"answerType\": \"numerical\", \"answer\": entry['answer'], \"mention\": str(entry['answer'])}\n",
    "        elif isinstance(entry['answer'], str):\n",
    "            entry['answer'] = {\"answerType\": \"text\", \"answer\": entry['answer'], \"mention\": entry['answer']}\n",
    "        elif not isinstance(entry['answer'], dict):\n",
    "            entry['answer'] = {\"answerType\": None, \"answer\": [], \"mention\": None}\n",
    "    else:\n",
    "        entry['answer'] = {\"answerType\": None, \"answer\": [], \"mention\": None}\n",
    "\n",
    "    formatted_entry = {\n",
    "        \"id\": entry[\"id\"],\n",
    "        \"question\": entry[\"question\"],\n",
    "        \"translations\": entry[\"translations\"],\n",
    "        \"answer\": entry[\"answer\"][\"mention\"],\n",
    "        \"answer_translations\": [entity[\"label\"] for entity in entry[\"answer\"][\"answer\"] if type(entry[\"answer\"][\"answer\"]) == list ]\n",
    "    }\n",
    "    formatted_data.append(formatted_entry)\n",
    "    \n",
    "\n",
    "\n",
    "with open('./mintaka_dev_extended_formatted.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(formatted_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# dev_file = path_to_data+\"mintaka_dev.json\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# dev_file = path_to_data+\"mintaka_dev_extended.json\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dev_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./mintaka_dev_extended_formatted.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: dev_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: dev_file})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "path_to_data = \"../dataset-generation/data/\"\n",
    "train_file = path_to_data+\"mintaka_train.json\"\n",
    "test_file = path_to_data+\"mintaka_test.json\"\n",
    "# dev_file = path_to_data+\"mintaka_dev.json\"\n",
    "# dev_file = path_to_data+\"mintaka_dev_extended.json\"\n",
    "dev_file = \"./mintaka_dev_extended_formatted.json\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": dev_file, \"validation\": dev_file})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we preprocess the data to prepare it for the Llama model\n",
    "The user needs to have been granted access to the Llama3.3 model on Huggingface while being logged in on the client system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre_trained_model = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess our dataset\n",
    "\n",
    "We need to separate the dataset into a \"question\" list and an \"answers\" list, which are symmetrically ordered and then encode them with the tokenizer which we loaded from llama3 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "\t# https://huggingface.co/transformers/v3.0.2/preprocessing.html\n",
    "\t# Look at 'Preprocessing pars of sentences'\n",
    "\t# encoded_input = tokenizer([]\"How old are you?\", \"what's your name?\"], [\"I'm 6 years old\", \"Magnus\"])\n",
    "\t# print(encoded_input)\n",
    "\n",
    "\tbatch_questions = [\n",
    "\t\t#LIST OF ALL QUESTIONS IN ORDER\n",
    "\t]\n",
    "\t\n",
    "\tbatch_answers = [\n",
    "\t\t#LIST OF ALL ANSWERS IN SAME ORDER AS QUESTIONS\n",
    "\t]\n",
    "\n",
    "\tencoded_inputs = tokenizer(batch_questions, batch_answers)\n",
    "\n",
    "\treturn {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map and test our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset.map(preprocess_data, bached=True,)\n",
    "\n",
    "print(processed_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'ache-dir=/home/user/tmp'\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -cache-dir=/home/user/tmp accelerate --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, get_scheduler, AutoModelForCausalLM\n",
    "\n",
    "def training_function():\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(pre_trained_model)\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "    train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "        train_dataloader, eval_dataloader, model, optimizer\n",
    "    )\n",
    "\n",
    "\n",
    "    num_epochs = 5\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        #do stuff here with our loss function and backwards propagation\n",
    "        for batch in train_dataloader:\n",
    "            outputs=model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
