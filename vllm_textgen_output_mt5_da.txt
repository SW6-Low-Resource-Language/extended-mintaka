INFO 04-16 08:54:57 __init__.py:207] Automatically detected platform cuda.
INFO 04-16 08:54:58 config.py:2444] Downcasting torch.float32 to torch.float16.
WARNING 04-16 08:54:58 config.py:2513] The model's config.json does not contain any of the following keys to determine the original maximum length of the model: ['max_position_embeddings', 'n_positions', 'max_seq_len', 'seq_length', 'model_max_length', 'max_target_positions', 'max_sequence_length', 'max_seq_length', 'seq_len']. Assuming the model's maximum length is 2048.
INFO 04-16 08:55:39 config.py:549] This model supports multiple tasks: {'classify', 'generate', 'score', 'embed', 'reward'}. Defaulting to 'generate'.
WARNING 04-16 08:55:39 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
WARNING 04-16 08:55:39 config.py:685] Async output processing is not supported on the current platform type cuda.
INFO 04-16 08:55:39 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='google/mt5-xl', speculative_config=None, tokenizer='google/mt5-xl', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/mt5-xl, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
[2025-04-16 08:55:44,467] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 04-16 08:55:46 cuda.py:229] Using Flash Attention backend.
INFO 04-16 08:55:46 model_runner.py:1110] Starting to load model google/mt5-xl...
